{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvSsNKP+t7b75h66aYK02U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vishvaish/Machine-Learning-Models/blob/main/Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression\n",
        "\n",
        "Logistic Regression is a statistical method used for binary classification, where the goal is to predict the probability of an event occurring based on given input variables. The output of logistic regression is a probability value between 0 and 1, which can be interpreted as the likelihood of the event occurring.\n",
        "\n"
      ],
      "metadata": {
        "id": "15SxQm-022XP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The formula for logistic regression is as follows:\n",
        "\n",
        "p = 1 / (1 + e^(-z))\n",
        "\n",
        "p = probability of the output variable taking the value 1\n",
        "\n",
        "z = b0 + b1x1 + b2x2 + ... + bn*xn\n",
        "\n",
        "b0, b1, b2, ..., bn = coefficients (weights) learned during training\n",
        "\n",
        "x1, x2, ..., xn = input variables\n",
        "\n",
        "The logistic function (also known as the sigmoid function) maps the output of the linear equation to a probability value between 0 and 1, which can be interpreted as the likelihood of the output variable taking the value 1.\n"
      ],
      "metadata": {
        "id": "9WJoSNYH3-qm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![logistic_regression.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "1j1YVP-X4nO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Intention:\n",
        "\n",
        "1. The code aims to implement logistic regression for binary classification for gender classification. \n",
        "\n",
        "2. The code loads the dataset using pandas, preprocesses the data by adding an intercept term to the features, initializes the model's parameters, trains the model using gradient descent, and finally evaluates the model's performance using the F1 score and by plotting the actual and predicted values."
      ],
      "metadata": {
        "id": "5X1hAmqc7bDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('/content/gender_classification_v7.csv')\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "XmpM5ezso7iv",
        "outputId": "ab7d0f50-fd02-4cd6-c375-a21948c54f2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   long_hair  forehead_width_cm  forehead_height_cm  nose_wide  nose_long  \\\n",
              "0          1               11.8                 6.1          1          0   \n",
              "1          0               14.0                 5.4          0          0   \n",
              "2          0               11.8                 6.3          1          1   \n",
              "3          0               14.4                 6.1          0          1   \n",
              "4          1               13.5                 5.9          0          0   \n",
              "\n",
              "   lips_thin  distance_nose_to_lip_long  gender  \n",
              "0          1                          1    Male  \n",
              "1          1                          0  Female  \n",
              "2          1                          1    Male  \n",
              "3          1                          1    Male  \n",
              "4          0                          0  Female  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6bc01e31-1e72-4db9-8793-1190f8591441\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>long_hair</th>\n",
              "      <th>forehead_width_cm</th>\n",
              "      <th>forehead_height_cm</th>\n",
              "      <th>nose_wide</th>\n",
              "      <th>nose_long</th>\n",
              "      <th>lips_thin</th>\n",
              "      <th>distance_nose_to_lip_long</th>\n",
              "      <th>gender</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>11.8</td>\n",
              "      <td>6.1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>5.4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>11.8</td>\n",
              "      <td>6.3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>14.4</td>\n",
              "      <td>6.1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>13.5</td>\n",
              "      <td>5.9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Female</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6bc01e31-1e72-4db9-8793-1190f8591441')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6bc01e31-1e72-4db9-8793-1190f8591441 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6bc01e31-1e72-4db9-8793-1190f8591441');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['gender'] = data['gender'].map({'Male':0,'Female':1})"
      ],
      "metadata": {
        "id": "hN2H5WSoy7KE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the sigmoid function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Define the cost function\n",
        "def cost_function(theta, X, y):\n",
        "    m = len(y)\n",
        "    h = sigmoid(X.dot(theta))\n",
        "    J = -(1/m) * (y.T.dot(np.log(h)) + (1-y).T.dot(np.log(1-h)))\n",
        "    return J\n",
        "\n",
        "# Define the gradient descent function\n",
        "def gradient_descent(theta, X, y, alpha, num_iters):\n",
        "    m = len(y)\n",
        "    J_history = np.zeros((num_iters, 1))\n",
        "\n",
        "    for i in range(num_iters):\n",
        "        h = sigmoid(X.dot(theta))\n",
        "        grad = (1/m) * X.T.dot(h - y)\n",
        "        theta = theta - alpha * grad\n",
        "        J_history[i] = cost_function(theta, X, y)\n",
        "\n",
        "    return theta, J_history\n",
        "\n",
        "# Split dataset into features and target\n",
        "X = data.iloc[:, :-1].values\n",
        "y = data.iloc[:, -1].values.reshape(-1, 1)\n",
        "\n",
        "# Add intercept term to X\n",
        "m = len(y)\n",
        "X = np.hstack((np.ones((m, 1)), X))\n",
        "\n",
        "# Initialize theta\n",
        "initial_theta = np.zeros((X.shape[1], 1))\n",
        "\n",
        "# Define hyperparameters\n",
        "alpha = 0.01\n",
        "num_iters = 1000\n",
        "\n",
        "# Train the model\n",
        "theta, J_history = gradient_descent(initial_theta, X, y, alpha, num_iters)\n",
        "\n",
        "# Predict\n",
        "y_pred = np.round(sigmoid(X.dot(theta)))\n",
        "\n",
        "# Compute F1 score\n",
        "tp = np.sum(np.logical_and(y_pred == 1, y == 1))\n",
        "fp = np.sum(np.logical_and(y_pred == 1, y == 0))\n",
        "fn = np.sum(np.logical_and(y_pred == 0, y == 1))\n",
        "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "print('F1 score:', f1_score)\n",
        "\n",
        "# Create actual and predicted dataframes\n",
        "df_actual = pd.DataFrame({'Actual': y.reshape(-1)})\n",
        "df_predicted = pd.DataFrame({'Predicted': y_pred.reshape(-1)})\n",
        "\n",
        "# Create dataframe for actual and predicted values\n",
        "df_result = pd.concat([df_actual, df_predicted], axis=1)\n",
        "print(df_result.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrTL929haaqg",
        "outputId": "79a3560d-dca6-41fd-bfa2-2489b3b54997"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score: 0.9466019417475727\n",
            "   Actual  Predicted\n",
            "0       0        0.0\n",
            "1       1        1.0\n",
            "2       0        0.0\n",
            "3       0        0.0\n",
            "4       1        1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference:\n",
        "\n",
        "1. The code defines a sigmoid function that returns the output of the sigmoid function for a given input.\n",
        "\n",
        "2. The code defines a cost function that calculates the cost of logistic regression for a given set of parameters, features, and target variables.\n",
        "\n",
        "3. The code defines a gradient descent function that uses the cost function and the sigmoid function to train the model and optimize the parameters using gradient descent.\n",
        "\n",
        "4. The code loads the dataset and preprocesses the data by adding an intercept term to the features.\n",
        "\n",
        "5. The code initializes the model's parameters, learning rate, and the number of iterations.\n",
        "\n",
        "6. The code trains the model using gradient descent.\n",
        "\n",
        "7. The code predicts the target variable using the trained model.\n",
        "The code evaluates the model's performance using the F1 score. "
      ],
      "metadata": {
        "id": "6hlK4yre8PLo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pros of Logistic Regression:\n",
        "\n",
        "1. Simplicity: Logistic regression is a relatively simple and straightforward algorithm compared to other machine learning models. It is easy to implement and interpret, making it a good choice for beginners.\n",
        "\n",
        "2. Interpretable results: Logistic regression provides interpretable results in terms of odds ratios and probabilities. It can help understand the relationship between the predictor variables and the likelihood of a particular outcome.\n",
        "\n",
        "3. Efficiency: Logistic regression tends to be computationally efficient and can handle large datasets with a relatively small number of predictor variables.\n",
        "\n",
        "4. Robustness to noise: Logistic regression is less prone to overfitting and can handle noisy data reasonably well. It is less affected by outliers compared to some other algorithms like decision trees.\n",
        "\n",
        "5. Feature importance: Logistic regression can provide information about the importance of each predictor variable in predicting the outcome. This can help in feature selection and identifying the most influential factors.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#Cons of Logistic Regression:\n",
        "\n",
        "1. Limited to linear relationships: Logistic regression assumes a linear relationship between the predictor variables and the log-odds of the outcome. It may not perform well when the relationship is highly nonlinear or complex.\n",
        "\n",
        "2. Assumptions of independence: Logistic regression assumes that the observations are independent of each other. If there is dependence or correlation among the observations, such as in time series or clustered data, logistic regression may produce biased results.\n",
        "\n",
        "3. Lack of flexibility: Logistic regression is a parametric model with a fixed functional form. It may not capture complex interactions or nonlinear patterns in the data as effectively as more flexible models like decision trees or neural networks.\n",
        "\n",
        "4. Sensitivity to outliers: Although logistic regression is generally robust to outliers, extreme outliers can still have a significant impact on the estimated coefficients and predictions.\n",
        "\n",
        "5. Imbalanced data: Logistic regression may struggle with imbalanced datasets, where the number of instances for each class is significantly different. The model tends to be biased towards the majority class, and additional techniques like resampling or adjusting class weights may be necessary.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vU1mz6HUqsTR"
      }
    }
  ]
}